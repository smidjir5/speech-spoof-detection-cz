<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dataset</title>
</head>
<body>
    <h2>Dataset making</h2>
    <p><a href="index.html">&lt;-- Back to main page</a></p>
    <p>
        For generating spoof utterances I used two bonafide datasets. First of them is in-house dataset "CTU Test" which contains 2169 phonetically rich utterances from 43 speakers. Second one is 
        czech subset from <a href="https://huggingface.co/datasets/facebook/voxpopuli" target="_blank">Voxpopuli</a> containg 62 hours of speech from 138 speakers recorded in Europian parliament.
        More datasets will be used. Too long and too short utterances were ignored.
    </p>

    <p><h3>Tools</h3></p>
    <p>
        <u>XTTS:</u> XTTS-v2 is a multilingual text-to-speech (TTS) and voice cloning model designed to synthesize high-quality speech in 17 languages while preserving speaker characteristics 
        from a short audio sample. It builds upon the Tortoise-TTS architecture with improvements in speaker conditioning, prosody modeling, and multilingual capability. The model consists of 
        a text encoder, which processes linguistic information; a speaker encoder, which extracts voice characteristics from a 6-second reference audio for zero-shot speaker adaptation; and 
        a decoder and vocoder, which generate 24 kHz waveforms with natural prosody and high fidelity. XTTS-v2 supports cross-lingual voice cloning, allowing a speaker’s voice to be 
        synthesized in languages they have not spoken. Key improvements include enhanced speaker conditioning, enabling multiple speaker references and interpolation, low-latency 
        inference (&lt;200 ms), and improved stability and quality across languages. These advancements make XTTS-v2 a state-of-the-art model for voice cloning, multilingual speech synthesis, 
        and personalized TTS applications.
        [Automatically compiled from: <a href="https://huggingface.co/coqui/XTTS-v2" target="_blank"> Hugging Face</a>] 
    </p>
    <p>
        <u>Free VC:</u> FreeVC is a text-free, one-shot voice conversion model based on the VITS framework. It employs a WavLM-based prior encoder with an information bottleneck to extract 
        speaker-independent content representations. A speaker encoder captures speaker-specific attributes, and a normalizing flow model enhances flexibility in 
        content conditioning. The system utilizes a spectrogram-resize-based augmentation technique to improve content disentanglement. FreeVC enables high-quality voice 
        conversion without requiring text annotations, outperforming prior methods in speaker similarity and naturalness.
        [Automatically compiled from: <a href="https://github.com/OlaWod/FreeVC/tree/main" target="_blank"> Git Hub</a>] 
    </p>
    <p>
        <u>Seed-VC:</u> Seed-VC is a zero-shot voice conversion model capable of transforming a source speaker’s voice to match a target speaker’s timbre without prior training on the 
        target voice. It extracts linguistic content using pre-trained models like XLSR-large or Whisper-small, while a speaker encoder generates embeddings from a short reference audio 
        sample (1–30 seconds). A decoder and vocoder (e.g., HIFI-GAN, BigVGAN) synthesize natural and intelligible speech. Seed-VC supports real-time processing with an algorithmic delay 
        of ~300ms and device-side delay of ~100ms, making it suitable for live applications. It enables fine-tuning with minimal data (as little as one utterance per speaker) and achieves 
        competitive performance against models like OpenVoice and CosyVoice, outperforming them in speaker similarity and intelligibility based on Speaker Embedding Cosine Similarity (SECS) 
        and Word Error Rate (WER). These features make Seed-VC an efficient solution for high-quality voice and singing voice conversion with minimal data and real-time capabilities.
        [Automatically compiled from: <a href="https://github.com/Plachtaa/seed-vc/tree/main" target="_blank"> Git Hub</a>] 








    </p>
    <p>
        <u>kNN-VC:</u> kNN-VC is a lightweight, text-free voice conversion model that leverages self-supervised speech representations and k-nearest neighbors (kNN) 
        regression for high-quality speaker adaptation. The model extracts speech features from both source and reference utterances using a pretrained self-supervised 
        model (e.g., HuBERT, WavLM). Each frame in the source representation is replaced by its closest match in the reference using kNN, ensuring content preservation while 
        transferring speaker characteristics. A pretrained vocoder reconstructs the waveform, producing natural speech. kNN-VC operates without adversarial training or parallel data, offering 
        a simple yet effective approach to voice conversion with strong speaker similarity and intelligibility. 
        [Automatically compiled from: <a href="https://github.com/bshall/knn-vc/tree/master"> Git Hub</a>] 
    </p>

    
  

    <h3>Metrics</h3>
    Several approaches exist for evaluating the quality of synthesized speech. The most commonly used metrics are those derived from the Mean Opinion Score (MOS), such 
    as the Perceptual Evaluation of Speech Quality (PESQ). However, a major limitation of these metrics is their reliance on a reference signal, which poses challenges when assessing synthetic speech.
    To address this issue, DNSMOS [<a href="https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS" target="_blank">code</a>, <a href="https://arxiv.org/pdf/2010.15258" target="_blank">paper</a>] 
    was considered, as it does not require a reference. While DNSMOS effectively evaluates the acoustic quality of speech, it does not account for the linguistic quality of Czech speech, particularly its 
    intelligibility. This aspect is crucial given that multilingual and English-trained tools were employed for speech synthesis. 

    <p>
        Currently, tools capable of assessing the similarity between artificial utterances and native Czech speakers are not readily available, and developing such tools would be excessively time-consuming. 
        Therefore, an alternative evaluation approach was devised, consisting of three steps. 
        First, a Czech Automatic Speech Recognition (ASR) model 
        <a href="https://huggingface.co/arampacha/wav2vec2-large-xlsr-czech" target="_blank">Wav2Vec2-Large-XLSR-53-Czech</a> was used to transcribe the synthesized speech. The underlying assumption is that 
        higher-quality and more natural speech would yield more accurate transcriptions.

        Second, the GPT-based language model <a href="https://huggingface.co/MU-NLPC/CzeGPT-2">CzeGPT-2</a> was employed to compute the perplexity of both the generated transcription and the reference transcription. 
        These perplexity values were then normalized by the number of words. Finally, the ratio of the two perplexities was calculated. A higher ratio indicates a more accurate transcription, suggesting better speech quality.
        
        
    </p>
    <p><h3>Resuts</h3></p>
    <p>Results were randomly select from subsets.</p>
    <p><h4>XTTS</h4></p>
    <p><h4>Free VC</h4></p>
    <p><h4>Seed-VC</h4></p>
    <p><h4>kNN-VC</h4></p>
    

</body>
</html>