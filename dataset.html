<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dataset</title>
</head>
<body>
    <style>
        table {
          border:1px solid black;
        }
        th,td {
            padding: 3px;
            text-align: center;
        }
        </style>
    <h2>Dataset making</h2>
    <p><a href="index.html">&lt;-- Back to main page</a></p>
    <p>
        For generating spoof utterances I used three bonafide datasets. First of them is in-house dataset "CTU Test" which contains 2169 phonetically rich utterances from 43 speakers. Second one is 
        czech subset from <a href="https://huggingface.co/datasets/facebook/voxpopuli" target="_blank">Voxpopuli</a> containg 62 hours of speech from 138 speakers recorded in Europian parliament. 
        The third dataset is the Czech subset of <a href="https://commonvoice.mozilla.org/cs/datasets" target="_blank">Common Voice</a>, which was incorporated to include utterances of lower acoustic quality, recorded by ordinary speakers in home environments.
        More datasets will be used. Too long and too short utterances were ignored.
    </p>

    <p><h3>Tools</h3></p>
    <p>For the purpose of dataset creation, only tools capable of utilizing speaker embeddings were selected, while tools trained on a single or a limited set of voices were excluded. Although this restriction reduces the number of available tools, it 
        enhances the realism of the dataset, as attackers are more likely to use tools that can handle speaker embeddings for generating spoofed speech.
    </p>
    <p>
        <u>XTTS:</u> XTTS-v2 is a multilingual text-to-speech (TTS) and voice cloning model designed to synthesize high-quality speech in 17 languages while preserving speaker characteristics 
        from a short audio sample. It builds upon the Tortoise-TTS architecture with improvements in speaker conditioning, prosody modeling, and multilingual capability. The model consists of 
        a text encoder, which processes linguistic information; a speaker encoder, which extracts voice characteristics from a 6-second reference audio for zero-shot speaker adaptation; and 
        a decoder and vocoder, which generate 24 kHz waveforms with natural prosody and high fidelity. XTTS-v2 supports cross-lingual voice cloning, allowing a speaker’s voice to be 
        synthesized in languages they have not spoken. Key improvements include enhanced speaker conditioning, enabling multiple speaker references and interpolation, low-latency 
        inference (&lt;200 ms), and improved stability and quality across languages. These advancements make XTTS-v2 a state-of-the-art model for voice cloning, multilingual speech synthesis, 
        and personalized TTS applications.
        [Automatically compiled from: <a href="https://huggingface.co/coqui/XTTS-v2" target="_blank"> Hugging Face</a>] 
    </p>
    <p>
        <u>Free VC:</u> FreeVC is a text-free, one-shot voice conversion model based on the VITS framework. It employs a WavLM-based prior encoder with an information bottleneck to extract 
        speaker-independent content representations. A speaker encoder captures speaker-specific attributes, and a normalizing flow model enhances flexibility in 
        content conditioning. The system utilizes a spectrogram-resize-based augmentation technique to improve content disentanglement. FreeVC enables high-quality voice 
        conversion without requiring text annotations, outperforming prior methods in speaker similarity and naturalness.
        [Automatically compiled from: <a href="https://github.com/OlaWod/FreeVC/tree/main" target="_blank"> Git Hub</a>] 
    </p>
    <p>
        <u>kNN-VC:</u> kNN-VC is a lightweight, text-free voice conversion model that leverages self-supervised speech representations and k-nearest neighbors (kNN) 
        regression for high-quality speaker adaptation. The model extracts speech features from both source and reference utterances using a pretrained self-supervised 
        model (e.g., HuBERT, WavLM). Each frame in the source representation is replaced by its closest match in the reference using kNN, ensuring content preservation while 
        transferring speaker characteristics. A pretrained vocoder reconstructs the waveform, producing natural speech. kNN-VC operates without adversarial training or parallel data, offering 
        a simple yet effective approach to voice conversion with strong speaker similarity and intelligibility. 
        [Automatically compiled from: <a href="https://github.com/bshall/knn-vc/tree/master"> Git Hub</a>] 
    </p>
    <p>
        <u>Seed-VC:</u> Seed-VC is a zero-shot voice conversion model capable of transforming a source speaker’s voice to match a target speaker’s timbre without prior training on the 
        target voice. It extracts linguistic content using pre-trained models like XLSR-large or Whisper-small, while a speaker encoder generates embeddings from a short reference audio 
        sample (1–30 seconds). A decoder and vocoder (e.g., HIFI-GAN, BigVGAN) synthesize natural and intelligible speech. Seed-VC supports real-time processing with an algorithmic delay 
        of ~300ms and device-side delay of ~100ms, making it suitable for live applications. It enables fine-tuning with minimal data (as little as one utterance per speaker) and achieves 
        competitive performance against models like OpenVoice and CosyVoice, outperforming them in speaker similarity and intelligibility based on Speaker Embedding Cosine Similarity (SECS) 
        and Word Error Rate (WER). These features make Seed-VC an efficient solution for high-quality voice and singing voice conversion with minimal data and real-time capabilities.
        [Automatically compiled from: <a href="https://github.com/Plachtaa/seed-vc/tree/main" target="_blank"> Git Hub</a>] 

    </p>
    <p>
        <u>Diff-HierVC:</u> Diff-HierVC is a diffusion-based hierarchical voice conversion system designed to enhance pitch accuracy and speaker adaptation in zero-shot scenarios. The architecture comprises 
        two primary components: DiffPitch and DiffVoice. DiffPitch generates fundamental frequency (F₀) contours aligned with the target speaker's style, while DiffVoice utilizes 
        these generated F₀ contours alongside a source-filter encoder to produce high-quality Mel-spectrograms that capture the desired voice characteristics. The source-filter encoder 
        effectively disentangles speech components, and the incorporation of a masked prior within the diffusion models further refines speaker adaptation. Experimental evaluations 
        demonstrate that Diff-HierVC outperforms existing methods in pitch generation and voice style transfer, achieving a character error rate (CER) of 0.83% and an equal error rate 
        (EER) of 3.29% in zero-shot voice conversion tasks. [Automatically compiled from: <a href="https://github.com/hayeong0/Diff-HierVC" target="_blank"> Git Hub</a>] 
    </p>


    <hr>
  
<p><h3></h3></p>
    <h3>Metrics</h3>
    Several approaches exist for evaluating the quality of synthesized speech. The most commonly used metrics are those derived from the Mean Opinion Score (MOS), such 
    as the Perceptual Evaluation of Speech Quality (PESQ). However, a major limitation of these metrics is their reliance on a reference signal, which poses challenges when assessing synthetic speech.
    To address this issue, DNSMOS [<a href="https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS" target="_blank">code</a>, <a href="https://arxiv.org/pdf/2010.15258" target="_blank">paper</a>] 
    was considered, as it does not require a reference. While DNSMOS effectively evaluates the acoustic quality of speech, it does not account for the linguistic quality of Czech speech, particularly its 
    intelligibility. This aspect is crucial given that multilingual and English-trained tools were employed for speech synthesis. 

    <p>
        Currently, tools capable of assessing the similarity between artificial utterances and native Czech speakers are not readily available, and developing such tools would be excessively time-consuming. 
        Therefore, an alternative evaluation approach was devised, consisting of three steps. 
        First, a Czech Automatic Speech Recognition (ASR) model 
        <a href="https://huggingface.co/mikr/whisper-large-v3-czech-cv13" target="_blank">Whisper-large-v3-czech-cv13</a> was used to transcribe the synthesized speech. The underlying assumption is that 
        higher-quality and more natural speech would yield more accurate transcriptions.

        Second, the GPT-based language model <a href="https://huggingface.co/MU-NLPC/CzeGPT-2">CzeGPT-2</a> was employed to compute the perplexity of both the generated transcription and the 
        reference transcription after removing non-alphabetical characters, converting numbers to words, and standardizing all text to lowercase. 
        These perplexity values were then normalized by the number of words. Finally, the ratio of the two perplexities was calculated. A higher ratio (around one) indicates a more accurate transcription, suggesting better speech quality.
        Only utterances with ratio between 0.2 and 1.2 were saved. Lower ratio indicates transcription with spelling errors or bad word selection by Whisper which indicates worse intelligibility of the utterance.
        A higher ratio indicates that Whisper does not correctly understand the speech and introduces improper words (which may not match the source transcription), but the sentece may make more sence than source transcription. 
        This can lead to a lower perplexity and, consequently, a higher ratio.
    </p>

    <hr>
    <p><h3>Generated data</h3></p>
    Still in progress. <br>
    <table>
        <tr>
            <th>Model</th>
            <th>Dataset</th>
            <th>Target utterances*</th>
            <th>Generated utterances</th>
            <th>Selected utterances</th>
            <th>Lenght of selected utterances [hours]</th>
        </tr>
        <tr>
            <td>XTTS</td>
            <td>Voxpopuli</td>
            <td>4</td>
            <td>2550</td>
            <td>2011</td>
            <td>3.10</td>
        </tr>
        <tr>
            <td>XTTS</td>
            <td>CTU Test</td>
            <td>4</td>
            <td>2000</td>
            <td>1653</td>
            <td>2.67</td>
        </tr>
        <tr>
            <td>XTTS</td>
            <td>CTU Test</td>
            <td>10</td>
            <td>2000</td>
            <td>1395</td>
            <td>2.43</td>
        </tr>
        <tr>
            <td>Free VC</td>
            <td>Voxpopuli</td>
            <td>1</td>
            <td>2846</td>
            <td>2069</td>
            <td>2.43</td>
        </tr>
        <tr>
            <td>Free VC</td>
            <td>CTU Test</td>
            <td>1</td>
            <td>1801</td>
            <td>1256</td>
            <td>2.43</td>
            
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>CTU Test</td>
            <td>5</td>
            <td>1801</td>
            <td>1134</td>
            <td>2.14</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>CTU Test</td>
            <td>10</td>
            <td>1801</td>
            <td>1209</td>
            <td>2.29</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>Voxpopuli</td>
            <td>10</td>
            <td>3335</td>
            <td>2456</td>
            <td>3.59</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>Common Voice</td>
            <td>10</td>
            <td>2333</td>
            <td>1698</td>
            <td>2.69</td>
        </tr>
        <tr>
            <td>Seed VC</td>
            <td>CTU Test</td>
            <td>1</td>
            <td>1801</td>
            <td>1298</td>
            <td>2.5</td>
        </tr>
        <tr>
            <td>Seed VC</td>
            <td>Common Voice</td>
            <td>1</td>
            <td>2550</td>
            <td>2058</td>
            <td>3.27</td>
        </tr>
        <tr>
            <td>Diff-HierVC</td>
            <td>CTU Test</td>
            <td>1</td>
            <td>1801</td>
            <td>1016</td>
            <td>2.12</td>
        </tr>
        <tr>
            <td>Diff-HierVC</td>
            <td>Voxpopuli</td>
            <td>1</td>
            <td>2846</td>
            <td>1963</td>
            <td>2.33</td>
        </tr>
        <tr>
            <td>Diff-HierVC</td>
            <td>Common Voice</td>
            <td>1</td>
            <td>3500</td>
            <td>2105</td>
            <td>3.36</td>
        </tr>
        

    </table>
    <p>* target utterances per one source utterance</p>
    <p>Overall 23 321 files with total duration of 37.35 hours were selected.</p>
    <p><small>Currently, the dataset is accessible only to members of the CTU Speech Lab team on Metacentrum as SP Spoof CS. A portion of the dataset that can be published with respect to copyright restrictions 
     will be publicly available by July.</small></p>

     <hr>
    <p><h3>Examples</h3></p>
    <p>Examples were randomly select from subsets.</p>
    <p><h4>XTTS</h4></p>
    <table>
        <tr>
            <th>Dataset</th>
            <th>Source</th>
            <th>Target</th>
            <th>Result</th>
        </tr>

        <tr>
            <td>CTU Test</td>
            <td>Větve stromů se ohýbaly pod tíhou sněhu, který <br> neúnavně padal celý den.</td>
            <td><audio controls src="samples/xtts_ctu_4/xtts_ctu_4_common_med_888_tar_1.wav"></audio><br>
                <audio controls src="samples/xtts_ctu_4/xtts_ctu_4_common_med_888_tar_2.wav"></audio><br>
                <audio controls src="samples/xtts_ctu_4/xtts_ctu_4_common_med_888_tar_3.wav"></audio><br>
                <audio controls src="samples/xtts_ctu_4/xtts_ctu_4_common_med_888_tar_4.wav"></audio><br>
            </td>
            <td>
                <audio controls src="samples/xtts_ctu_4/xtts_ctu_4_common_med_888.wav"></audio>
            </td>
        </tr>


        

    </table>

    <p><h4>Free VC</h4></p>
    <table>
        <tr>
            <th>Dataset</th>
            <th>Source</th>
            <th>Target</th>
            <th>Result</th>
        </tr>
        <tr>
            <td>Voxpopuli</td>
            <td><audio controls src="samples/free_vc/fvc_voxpopuli_1_src.wav"></audio></td>
            <td><audio controls src="samples/free_vc/fvc_voxpopuli_1_tar.wav"></audio></td>
            <td><audio controls src="samples/free_vc/fvc_voxpopuli_1.wav"></audio></td>
              
        </tr>
        <tr>
            <td>Voxpopuli</td>
            <td><audio controls src="samples/free_vc/fvc_voxpopuli_3_src.wav"></audio></td>
            <td><audio controls src="samples/free_vc/fvc_voxpopuli_3_tar.wav"></audio></td>
            <td><audio controls src="samples/free_vc/fvc_voxpopuli_3.wav"></audio></td>
              
        </tr>

    </table>

    <p><h4>Seed-VC</h4></p>
    <table>
        <tr>
            <th>Dataset</th>
            <th>Source</th>
            <th>Target</th>
            <th>Result</th>
        </tr>
        <tr>
            <td>Common Voice</td>
            <td><audio controls src="samples/seed_vc/seed_common_401_src.mp3"></audio></td>
            <td><audio controls src="samples/seed_vc/seed_common_401_tar.mp3"></audio></td>
            <td><audio controls src="samples/seed_vc/seed_common_401.wav"></audio></td>
        </tr>
        <tr>
            <td>CTU Test</td>
            <td><audio controls src="samples/seed_vc/seed_ctu_704_src.wav"></audio></td>
            <td><audio controls src="samples/seed_vc/seed_ctu_704_tar.wav"></audio></td>
            <td><audio controls src="samples/seed_vc/seed_ctu_704.wav"></audio></td>
        </tr>
        <tr>
            <td>CTU Test</td>
            <td><audio controls src="samples/seed_vc/seed_ctu_705_src.wav"></audio></td>
            <td><audio controls src="samples/seed_vc/seed_ctu_705_tar.wav"></audio></td>
            <td><audio controls src="samples/seed_vc/seed_ctu_705.wav"></audio></td>
        </tr>

    </table>

    <p><h4>kNN-VC</h4></p>
    <table>
        <tr>
            <th>Dataset</th>
            <th>Source</th>
            <th>Target</th>
            <th>Result</th>
        </tr>
        <tr>
            <td>CTU Test</td>
            <td><audio controls src="samples/knn/knn_ctu_10_1_src.wav"></audio></td>
            <td><audio controls src="samples/knn/knn_ctu_10_1_tar_1.wav"></audio><br>
                <audio controls src="samples/knn/knn_ctu_10_1_tar_2.wav"></audio><br>
                <audio controls src="samples/knn/knn_ctu_10_1_tar_3.wav"></audio><br>
                <b>...</b>
            
            </td>
            <td><audio controls src="samples/knn/knn_ctu_10_1.wav"></audio></td>
        </tr>
        <tr>
            <td>Voxpopuli</td>
            <td><audio controls src="samples/knn/knn_voxpopuli_10_1_src.wav"></audio></td>
            <td><audio controls src="samples/knn/knn_voxpopuli_10_1_tar_1.wav"></audio><br>
                <audio controls src="samples/knn/knn_voxpopuli_10_1_tar_2.wav"></audio><br>
                <audio controls src="samples/knn/knn_voxpopuli_10_1_tar_3.wav"></audio><br>
                <b>...</b>
            
            </td>
            <td><audio controls src="samples/knn/knn_voxpopuli_10_1.wav"></audio></td>
        </tr>
    </table>
    <p><h4>Diff-Hier VC</h4></p>
    <table>
        <tr>
            <th>Dataset</th>
            <th>Source</th>
            <th>Target</th>
            <th>Result</th>
        </tr>
        <tr>
            <td>CTU Test</td>
            <td><audio controls src="samples/diff/diff_ctu_6_src.wav"></audio></td>
            <td><audio controls src="samples/diff/diff_ctu_6_tar.wav"></audio></td>
            <td><audio controls src="samples/diff/diff_ctu_6.wav"></audio></td>

        </tr>
        <tr>
            <td>Common Voice</td>
            <td><audio controls src="samples/diff/diff_common_5_src.mp3"></audio></td>
            <td><audio controls src="samples/diff/diff_common_5_tar.mp3"></audio></td>
            <td><audio controls src="samples/diff/diff_common_5.wav"></audio></td>
        </tr>
    </table>

    <hr>
    <p><h3>Artifacts</h3></p>

    <p>
        One unexpected artifact occurs when the CTU Test dataset is used for XTTS. In this dataset, utterances include spoken representations of punctuation marks, such 
        as "comma" ("čárka" in Czech) and "dot" ("tečka" in Czech), instead of relying on natural intonation to indicate sentence boundaries. As a result, an artifact commonly 
        appears at the end of many generated utterances, producing a sound resembling "tečka".
        This suggests that the speaker embedding extractor for XTTS interprets these spoken punctuation marks as part of the speaker’s vocal characteristics, leading to their 
        unintended presence at the end of synthesized sentences. 
        Although this artifact may be unpleasant for the listener, it does not negatively impact the effectiveness of speech in the task of speech spoof detection
    </p>
    <p>
        As previously mentioned, another artifact that occurs is the presence of English-like vowels and consonants, particularly in utterances generated by Free VC, which can only process a single reference. A common issue arises with uniquely Czech consonants, such as "ř", which has no direct equivalent in English. Additionally, vowel pronunciation often exhibits slight English influences.

        While some generated utterances are free of artifacts, others are noticeably distorted. The underlying cause of this issue lies in the architecture of the synthesis systems and 
        their training datasets, which are primarily in English. To address this, the previously described evaluation metric was applied. Although some selected utterances may still contain 
        distortions, this is not necessarily problematic, as such artifacts could help a spoof detection system learn to recognize potential distortions introduced by English-trained TTS and 
        VC models to Czech speech.
    </p>
    
    <hr>

    <h3>Dataset</h3>
    <p>
        The spoof partition of the final dataset is composed of all selected utterances described in the Generated Data section. For the initial tests, the goal is to create a balanced dataset, which 
        is often a challenge in bona fide/spoof datasets. To achieve this, utterances containing between 5 to 15 words will be selected from the three datasets 
        (CTU Test, VoxPopuli, and Common Voice) in a quantity of approximately 23,000 to match the size of the spoof partition.
    </p>
    <p>
        If necessary, 11 hours from the Czech portion of the MLAAD dataset can also be utilized.
    </p>
</body>
</html>