<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dataset</title>
</head>
<body>
    <style>
        table {
          border:1px solid black;
        }
        th,td {
            padding: 3px;
            text-align: center;
        }
        </style>
    <h2>Dataset making</h2>
    <p><a href="index.html">&lt;-- Back to main page</a></p>
    <p>
        For generating spoof utterances I used three bonafide datasets. First of them is in-house dataset "CTU Test" which contains 2169 phonetically rich utterances from 43 speakers. Second one is 
        czech subset from <a href="https://huggingface.co/datasets/facebook/voxpopuli" target="_blank">Voxpopuli</a> containg 62 hours of speech from 138 speakers recorded in Europian parliament. 
        The third dataset is the Czech subset of <a href="https://commonvoice.mozilla.org/cs/datasets" target="_blank">Common Voice</a>, which was incorporated to include utterances of lower 
        acustic quality.
        More datasets will be used. Too long and too short utterances were ignored.
    </p>

    <p><h3>Tools</h3></p>
    <p>
        <u>XTTS:</u> XTTS-v2 is a multilingual text-to-speech (TTS) and voice cloning model designed to synthesize high-quality speech in 17 languages while preserving speaker characteristics 
        from a short audio sample. It builds upon the Tortoise-TTS architecture with improvements in speaker conditioning, prosody modeling, and multilingual capability. The model consists of 
        a text encoder, which processes linguistic information; a speaker encoder, which extracts voice characteristics from a 6-second reference audio for zero-shot speaker adaptation; and 
        a decoder and vocoder, which generate 24 kHz waveforms with natural prosody and high fidelity. XTTS-v2 supports cross-lingual voice cloning, allowing a speaker’s voice to be 
        synthesized in languages they have not spoken. Key improvements include enhanced speaker conditioning, enabling multiple speaker references and interpolation, low-latency 
        inference (&lt;200 ms), and improved stability and quality across languages. These advancements make XTTS-v2 a state-of-the-art model for voice cloning, multilingual speech synthesis, 
        and personalized TTS applications.
        [Automatically compiled from: <a href="https://huggingface.co/coqui/XTTS-v2" target="_blank"> Hugging Face</a>] 
    </p>
    <p>
        <u>Free VC:</u> FreeVC is a text-free, one-shot voice conversion model based on the VITS framework. It employs a WavLM-based prior encoder with an information bottleneck to extract 
        speaker-independent content representations. A speaker encoder captures speaker-specific attributes, and a normalizing flow model enhances flexibility in 
        content conditioning. The system utilizes a spectrogram-resize-based augmentation technique to improve content disentanglement. FreeVC enables high-quality voice 
        conversion without requiring text annotations, outperforming prior methods in speaker similarity and naturalness.
        [Automatically compiled from: <a href="https://github.com/OlaWod/FreeVC/tree/main" target="_blank"> Git Hub</a>] 
    </p>
    <p>
        <u>kNN-VC:</u> kNN-VC is a lightweight, text-free voice conversion model that leverages self-supervised speech representations and k-nearest neighbors (kNN) 
        regression for high-quality speaker adaptation. The model extracts speech features from both source and reference utterances using a pretrained self-supervised 
        model (e.g., HuBERT, WavLM). Each frame in the source representation is replaced by its closest match in the reference using kNN, ensuring content preservation while 
        transferring speaker characteristics. A pretrained vocoder reconstructs the waveform, producing natural speech. kNN-VC operates without adversarial training or parallel data, offering 
        a simple yet effective approach to voice conversion with strong speaker similarity and intelligibility. 
        [Automatically compiled from: <a href="https://github.com/bshall/knn-vc/tree/master"> Git Hub</a>] 
    </p>
    <p>
        <u>Seed-VC:</u> Seed-VC is a zero-shot voice conversion model capable of transforming a source speaker’s voice to match a target speaker’s timbre without prior training on the 
        target voice. It extracts linguistic content using pre-trained models like XLSR-large or Whisper-small, while a speaker encoder generates embeddings from a short reference audio 
        sample (1–30 seconds). A decoder and vocoder (e.g., HIFI-GAN, BigVGAN) synthesize natural and intelligible speech. Seed-VC supports real-time processing with an algorithmic delay 
        of ~300ms and device-side delay of ~100ms, making it suitable for live applications. It enables fine-tuning with minimal data (as little as one utterance per speaker) and achieves 
        competitive performance against models like OpenVoice and CosyVoice, outperforming them in speaker similarity and intelligibility based on Speaker Embedding Cosine Similarity (SECS) 
        and Word Error Rate (WER). These features make Seed-VC an efficient solution for high-quality voice and singing voice conversion with minimal data and real-time capabilities.
        [Automatically compiled from: <a href="https://github.com/Plachtaa/seed-vc/tree/main" target="_blank"> Git Hub</a>] 

    </p>


    
  
<p><h3></h3></p>
    <h3>Metrics</h3>
    Several approaches exist for evaluating the quality of synthesized speech. The most commonly used metrics are those derived from the Mean Opinion Score (MOS), such 
    as the Perceptual Evaluation of Speech Quality (PESQ). However, a major limitation of these metrics is their reliance on a reference signal, which poses challenges when assessing synthetic speech.
    To address this issue, DNSMOS [<a href="https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS" target="_blank">code</a>, <a href="https://arxiv.org/pdf/2010.15258" target="_blank">paper</a>] 
    was considered, as it does not require a reference. While DNSMOS effectively evaluates the acoustic quality of speech, it does not account for the linguistic quality of Czech speech, particularly its 
    intelligibility. This aspect is crucial given that multilingual and English-trained tools were employed for speech synthesis. 

    <p>
        Currently, tools capable of assessing the similarity between artificial utterances and native Czech speakers are not readily available, and developing such tools would be excessively time-consuming. 
        Therefore, an alternative evaluation approach was devised, consisting of three steps. 
        First, a Czech Automatic Speech Recognition (ASR) model 
        <a href="https://huggingface.co/arampacha/wav2vec2-large-xlsr-czech" target="_blank">Wav2Vec2-Large-XLSR-53-Czech</a> was used to transcribe the synthesized speech. The underlying assumption is that 
        higher-quality and more natural speech would yield more accurate transcriptions.

        Second, the GPT-based language model <a href="https://huggingface.co/MU-NLPC/CzeGPT-2">CzeGPT-2</a> was employed to compute the perplexity of both the generated transcription and the reference transcription. 
        These perplexity values were then normalized by the number of words. Finally, the ratio of the two perplexities was calculated. A higher ratio indicates a more accurate transcription, suggesting better speech quality.
        Tris metric was used for utterances generated by XTTS and Free VC where the occurece of non czech sounds was frequent.
    </p>

    <p><h3>Generated data</h3></p>
    Still in progress. <br>
    <table>
        <tr>
            <th>Model</th>
            <th>Dataset</th>
            <th>Target utterances*</th>
            <th>Generated utterances</th>
            <th>Selected utterances</th>
            <th>Lenght of selected utterances [hours]</th>
        </tr>
        <tr>
            <td>XTTS</td>
            <td>Voxpopuli</td>
            <td>6</td>
            <td>2846</td>
        </tr>
        <tr>
            <td>XTTS</td>
            <td>CTU Test</td>
            <td>4</td>
            <td>2000</td>
        </tr>
        <tr>
            <td>XTTS</td>
            <td>CTU Test</td>
            <td>10</td>
            <td>2000</td>
        </tr>
        <tr>
            <td>Free VC</td>
            <td>Voxpopuli</td>
            <td>1</td>
            <td>2846</td>
        </tr>
        <tr>
            <td>Free VC</td>
            <td>CTU Test</td>
            <td>1</td>
            <td>1801</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>CTU Test</td>
            <td>5</td>
            <td>1801</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>CTU Test</td>
            <td>10</td>
            <td>1801</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>Voxpopuli</td>
            <td>10</td>
            <td>3335</td>
        </tr>
        <tr>
            <td>kNN VC</td>
            <td>Common Voice</td>
            <td>10</td>
            <td>2333</td>
        </tr>
        <tr>
            <td>Seed VC</td>
            <td>CTU Test</td>
            <td>1</td>
            <td>1801</td>
        </tr>

    </table>
    <p>* target utterances per one source utterance</p>
    <p><h3>Examples</h3></p>
    <p>Examples were randomly select from subsets.</p>
    <p><h4>XTTS</h4></p>
    <p><h4>Free VC</h4></p>
    <p><h4>Seed-VC</h4></p>
    <p><h4>kNN-VC</h4></p>
    <p><h3>Artifacts</h3></p>

    <p>
        One unexpected artifact occurs when the CTU Test dataset is used for XTTS. In this dataset, utterances include spoken representations of punctuation marks, such 
        as "comma" ("čárka" in Czech) and "dot" ("tečka" in Czech), instead of relying on natural intonation to indicate sentence boundaries. As a result, an artifact commonly 
        appears at the end of many generated utterances, producing a sound resembling "tečka".
        This suggests that the speaker embedding extractor for XTTS interprets these spoken punctuation marks as part of the speaker’s vocal characteristics, leading to their 
        unintended presence at the end of synthesized sentences. 
        Although this artifact may be unpleasant for the listener, it does not negatively impact the effectiveness of speech in the task of speech spoof detection
    </p>
    <p>
        As previously mentioned, another artifact that occurs is the presence of English-like vowels and consonants, particularly in utterances generated by Free VC, which can only process a single reference. A common issue arises with uniquely Czech consonants, such as "ř", which has no direct equivalent in English. Additionally, vowel pronunciation often exhibits slight English influences.

        While some generated utterances are free of artifacts, others are noticeably distorted. The underlying cause of this issue lies in the architecture of the synthesis systems and 
        their training datasets, which are primarily in English. To address this, the previously described evaluation metric was applied. Although some selected utterances may still contain 
        distortions, this is not necessarily problematic, as such artifacts could help a spoof detection system learn to recognize potential distortions introduced by English-trained TTS and 
        VC models to Czech speech.
    </p>
    

</body>
</html>